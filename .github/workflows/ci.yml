name: ci

on:
  workflow_dispatch:
    inputs:
      logLevel:
        description: 'Log level'
        required: true
        default: 'info'
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]
        node-version: ["18.x"]

    steps:
    - uses: actions/checkout@v3
      with:
        submodules: recursive

    - name: Install build packages, required run dependencies
      run: |
        sudo ACCEPT_EULA=Y apt-get update
        sudo ACCEPT_EULA=Y apt-get upgrade
        sudo apt-get install git curl software-properties-common build-essential libopenblas-dev ninja-build

    - name: Download model weights
      run: |
        curl -s -L --remote-name-all https://7b-llm-models-1302315972.cos.ap-beijing.myqcloud.com/7B.zip https://7b-llm-models-1302315972.cos.ap-beijing.myqcloud.com/OpenLlama7B.zip https://7b-llm-models-1302315972.cos.ap-beijing.myqcloud.com/Panda7BInstr.zip
        unzip -q 7B.zip
        mv 7B ../llama-web-server/llama-cpp-python/vendor/llama.cpp/models/
        mkdir temp
        unzip -q OpenLlama7B.zip -d temp
        mv temp/OpenLlama7B ../llama-web-server/llama-cpp-python/vendor/llama.cpp/models/
        unzip -q Panda7BInstr.zip -d temp
        mv temp/Panda7BInstr ../llama-web-server/llama-cpp-python/vendor/llama.cpp/models/
        rm -rf 7B.zip OpenLlama7B.zip Panda7BInstr.zip temp
      working-directory: models
    
    - uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install python packages
      run: 
        python -m pip install --upgrade pip pytest cmake scikit-build setuptools fastapi uvicorn sse-starlette
        
    - name: Build backend server and backend docker image
      run: |
        LIBOPENBLAS=1 python3 setup.py develop
        cd vendor/llama.cpp
        make libllama.so
        mv libllama.so ../../llama_cpp
        docker build -t divinaventi/llama-web-server .
        cd ../..
      working-directory: llama-web-server/llama-cpp-python
      
    - name: Run backend server in background and wait for it to start
      run: |
        nohup python3 -m llama_cpp.server --model vendor/llama.cpp/models/7B/ggml-model-q4_0.bin
        echo $! > ../../backend.pid
        sleep 20
      working-directory: llama-web-server/llama-cpp-python
      
    - name: Test backend server
      run: |
        curl http://localhost:8000 -X POST -d @postreq.json
      
    - name: Install node dependencies
      uses: actions/setup-node@v3
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
        cache-dependency-path: llama-frontend/package-lock.json
    
    - name: Build frontend docker image
      run: |
        docker build -t divinaventi/llama-frontend:prodv2 .
    
    - name: Run frontend in background and wait for it to start
      run: |
        nohup npm ci
        echo $! > ../../frontend.pid
        sleep 20
        
    - name: Build frontend docker image
      run: 
        docker 

    - name: Test frontend server
      run: curl -X POST -H "Content-Type:application/json" -d '{"id":"chatcmpl-3438af13-99c4-4a43-a064-9a4b7592d937", "object":"chat.completion", "created":1686589302, "model":"../../models/ggml-model-q4_0.bin", "choices":[{"index":0, "message":{"role":"assistant", "content":"Paris is the capital of France."}, "finish_reason":"stop"}], "usage":{"prompt_tokens":84, "completion_tokens":12, "total_tokens":96}}' -H "access-control-allow-credentials:true" -H "access-control-allow-origin:*" -H "content-length:337" -H "content-type:application/json" -H "date:Mon, 12 Jun 2023 17:01:41 GMT" -H "server:uvicorn" http://localhost

    - name: Save build state for demo
      uses: actions/cache/save@v3
      id: restore-build
      with:
        path: ./*
        key: ${{ github.sha }}

    - name: Finish message, final cleanup and termination
      run: 
        kill -9 `cat backend.pid`
        kill -9 `cat frontend.pid`
        rm backend.pid
        rm frontend.pid
        echo "All good!"
        
        
  demo:
    runs-on: ubuntu-latest
    needs: [build]

    steps:
    - uses: actions/checkout@v3
      with:
        submodules: recursive

    - name: Load build state for demo
      uses: actions/cache/restore@v3
      id: restore-build
      with:
        path: ./*
        key: ${{ github.sha }}
    
    - name: Start web app composer in background
      run: |
        nohup docker compose up
        sleep 30
      
    - name: Serve curl 
      run: |
        curl -X POST -H "Content-Type:application/json" -H "Accept: application/json" -d '{"data":{"messages":[{"role":"system","content":"You are a helpful assistant. Provide helpful and informative responses in a concise and complete manner. Please avoid using conversational tags and only reply in full sentences. Ensure that your answers are presented directly and without the use of 'Human:' or '###'. Thank you for your cooperation!"},{"role":"user","content":"What was the significance of Joseph Weizenbaum's ELIZA program?"}]}}' http://localhost:8000/v1/chat/completions
        echo "Navigate to the url http://localhost:8000/v1/chat/completions in any web browser to view the demo"
        sleep 15
        docker compose kill


        
    
