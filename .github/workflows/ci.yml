name: ci

on:
  workflow_dispatch:
    inputs:
      logLevel:
        description: 'Log level'
        required: true
        default: 'info'
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]
        node-version: ["18.x"]

    steps:
    - uses: actions/checkout@v3
      with:
        submodules: recursive

    - name: Install build packages, required run dependencies
      run: |
        sudo ACCEPT_EULA=Y apt-get update
        sudo ACCEPT_EULA=Y apt-get upgrade
        sudo apt-get install git curl software-properties-common build-essential libopenblas-dev ninja-build

    - name: Download model weights
      run: |
        mkdir 7B OpenLlama7B Panda7BInstr
        curl -s -L --remote-name-all https://7b-llm-models-1302315972.cos.ap-beijing.myqcloud.com/7B.zip https://7b-llm-models-1302315972.cos.ap-beijing.myqcloud.com/OpenLlama7B.zip https://7b-llm-models-1302315972.cos.ap-beijing.myqcloud.com/Panda7BInstr.zip
        ls -l
        unzip -q 7B.zip -d 7B
        mv 7B/7B ../llama-web-server/llama-cpp-python/vendor/llama.cpp/models/
        unzip -q OpenLlama7B.zip -d OpenLlama7B
        ls OpenLlama7B
        mv OpenLlama7B/OpenLlama7B ../llama-web-server/llama-cpp-python/vendor/llama.cpp/models/
        unzip -q Panda7BInstr.zip -d Panda7BInstr
        mv Panda7BInstr/Panda7BInstr ../llama-web-server/llama-cpp-python/vendor/llama.cpp/models/
        rm -rf 7B.zip OpenLlama7B.zip Panda7BInstr.zip
        rm -rf 7B OpenLlama7B Panda7BInstr
      working-directory: models
    
    - uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install python packages
      run: 
        python -m pip install --upgrade pip pytest cmake scikit-build setuptools fastapi uvicorn sse-starlette
        
    - name: Build backend server
      run: |
        LIBOPENBLAS=1 python3 setup.py develop
        cd vendor/llama.cpp
        make libllama.so
        mv libllama.so ../../llama_cpp
        cd ../..
      working-directory: llama-web-server/llama-cpp-python
      
    - name: Run backend server
      run: |
        ls vendor/llama.cpp/models
        echo "tested1"
        ls vendor/llama.cpp/models/7B/
        python3 -m llama_cpp.server --model vendor/llama.cpp/models/7B/ggml-model-q4_0.bin
      working-directory: llama-web-server/llama-cpp-python
      
    - name: Test backend server
      run:
        curl http://localhost:8000 -X POST -d @postreq.json
      
    - uses: actions/setup-node@v3
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
        cache-dependency-path: llama-frontend/package-lock.json
    - name: Install node dependencies
      run:
        npm ci

    - name: Test frontend server
      run: curl -X POST -H "Content-Type:application/json" -d '{"id":"chatcmpl-3438af13-99c4-4a43-a064-9a4b7592d937", "object":"chat.completion", "created":1686589302, "model":"../../models/ggml-model-q4_0.bin", "choices":[{"index":0, "message":{"role":"assistant", "content":"Paris is the capital of France."}, "finish_reason":"stop"}], "usage":{"prompt_tokens":84, "completion_tokens":12, "total_tokens":96}}' -H "access-control-allow-credentials:true" -H "access-control-allow-origin:*" -H "content-length:337" -H "content-type:application/json" -H "date:Mon, 12 Jun 2023 17:01:41 GMT" -H "server:uvicorn" http://localhost

    - name: Finish message
      run: 
        echo "All good!"
